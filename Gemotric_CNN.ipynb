{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1"
      ],
      "metadata": {
        "id": "TuBZeOgYN0kQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KCrxRtrKNz1e"
      },
      "outputs": [],
      "source": [
        "class GeometricDropout(nn.Module):\n",
        "    def _init_(self, dropout_start=0.1, dropout_end=0.5, total_epochs=50):\n",
        "        super(GeometricDropout, self)._init_()\n",
        "        self.dropout_start = dropout_start\n",
        "        self.dropout_end = dropout_end\n",
        "        self.total_epochs = total_epochs\n",
        "\n",
        "    def forward(self, x, epoch, training):\n",
        "        if not training:\n",
        "            return x\n",
        "        dropout_rate = self.get_dropout_rate(epoch)\n",
        "        noise = torch.empty_like(x).exponential_(dropout_rate)\n",
        "        return x * (1 - noise)\n",
        "\n",
        "    def get_dropout_rate(self, epoch):\n",
        "        return self.dropout_start + (self.dropout_end - self.dropout_start) * (epoch / self.total_epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2"
      ],
      "metadata": {
        "id": "Qe_LoWYbODoK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NetStandard(nn.Module):\n",
        "    def _init_(self, dropout_rate=0.5):\n",
        "        super(NetStandard, self)._init_()\n",
        "        self.conv1 = nn.Conv2d(3, 96, 5, padding=2)\n",
        "        self.conv2 = nn.Conv2d(96, 128, 5, padding=2)\n",
        "        self.conv3 = nn.Conv2d(128, 256, 5, padding=2)\n",
        "        self.pool = nn.MaxPool2d(3, 2)\n",
        "        self.fc1 = nn.Linear(256 * 3 * 3, 2048)\n",
        "        self.fc2 = nn.Linear(2048, 2048)\n",
        "        self.fc3 = nn.Linear(2048, 10)\n",
        "        self.dropout_rate = dropout_rate\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = self.pool(F.relu(self.conv3(x)))\n",
        "        x = x.view(-1, 256 * 3 * 3)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.dropout(x, p=self.dropout_rate, training=self.training)\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = F.dropout(x, p=self.dropout_rate, training=self.training)\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "uPsfKRzaOEEZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3"
      ],
      "metadata": {
        "id": "Tbnu4OlFOFhf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NetAdaptive(nn.Module):\n",
        "    def _init_(self, dropout_start=0.1, dropout_end=0.5, total_epochs=50):\n",
        "        super(NetAdaptive, self)._init_()\n",
        "        self.conv1 = nn.Conv2d(3, 96, 5, padding=2)\n",
        "        self.conv2 = nn.Conv2d(96, 128, 5, padding=2)\n",
        "        self.conv3 = nn.Conv2d(128, 256, 5, padding=2)\n",
        "        self.pool = nn.MaxPool2d(3, 2)\n",
        "        self.fc1 = nn.Linear(256 * 3 * 3, 2048)\n",
        "        self.fc2 = nn.Linear(2048, 2048)\n",
        "        self.fc3 = nn.Linear(2048, 10)\n",
        "        self.geometric_dropout = GeometricDropout(dropout_start, dropout_end, total_epochs)\n",
        "\n",
        "    def forward(self, x, epoch):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = self.pool(F.relu(self.conv3(x)))\n",
        "        x = x.view(-1, 256 * 3 * 3)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.geometric_dropout(x, epoch, self.training)  # Geometric dropout\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.geometric_dropout(x, epoch, self.training)  # Geometric dropout\n",
        "        x = self.fc3(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "f4KvVn2sOGHk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4"
      ],
      "metadata": {
        "id": "d5hj958NOIxv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_sparsity(tensor):\n",
        "    return 100.0 * float(torch.sum(tensor == 0)) / tensor.numel()\n",
        "\n",
        "def track_sparsity(net, dataloader, epoch, adaptive):\n",
        "    sparsity = 0.0\n",
        "    count = 0\n",
        "    net.eval()\n",
        "    with torch.no_grad():\n",
        "        for data in dataloader:\n",
        "            images, labels = data\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            if adaptive:\n",
        "                outputs = net(images, epoch)\n",
        "            else:\n",
        "                outputs = net(images)\n",
        "            sparsity += calculate_sparsity(outputs)\n",
        "            count += 1\n",
        "    return sparsity / count\n",
        "\n",
        "def train_and_record_errors(net, trainloader, testloader, criterion, optimizer, num_epochs=50, adaptive=False):\n",
        "    net.to(device)\n",
        "    train_errors = []\n",
        "    test_errors = []\n",
        "    train_sparsity = []\n",
        "    test_sparsity = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        net.train()\n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            inputs, labels = data\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            if adaptive:\n",
        "                outputs = net(inputs, epoch)\n",
        "            else:\n",
        "                outputs = net(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        # Calculate train and test error and sparsity\n",
        "        train_error = calculate_classification_error(net, trainloader, epoch, adaptive)\n",
        "        test_error = calculate_classification_error(net, testloader, epoch, adaptive)\n",
        "        train_errors.append(train_error)\n",
        "        test_errors.append(test_error)\n",
        "\n",
        "        train_sparsity.append(track_sparsity(net, trainloader, epoch, adaptive))\n",
        "        test_sparsity.append(track_sparsity(net, testloader, epoch, adaptive))\n",
        "\n",
        "        print(f'Epoch {epoch + 1}, Loss: {running_loss / len(trainloader)}, Train Error: {train_error}, Test Error: {test_error}')\n",
        "        print(f'Epoch {epoch + 1}, Train Sparsity: {train_sparsity[-1]}%, Test Sparsity: {test_sparsity[-1]}%')\n",
        "\n",
        "    return train_errors, test_errors, train_sparsity, test_sparsity\n"
      ],
      "metadata": {
        "id": "Qh9fR__VOJwd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5"
      ],
      "metadata": {
        "id": "WFlepa6uOOBK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_sparsity(tensor):\n",
        "    return 100.0 * float(torch.sum(tensor == 0)) / tensor.numel()\n",
        "\n",
        "def track_sparsity(net, dataloader, epoch, adaptive):\n",
        "    sparsity = 0.0\n",
        "    count = 0\n",
        "    net.eval()\n",
        "    with torch.no_grad():\n",
        "        for data in dataloader:\n",
        "            images, labels = data\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            if adaptive:\n",
        "                outputs = net(images, epoch)\n",
        "            else:\n",
        "                outputs = net(images)\n",
        "            sparsity += calculate_sparsity(outputs)\n",
        "            count += 1\n",
        "    return sparsity / count\n",
        "\n",
        "def train_and_record_errors(net, trainloader, testloader, criterion, optimizer, num_epochs=50, adaptive=False):\n",
        "    net.to(device)\n",
        "    train_errors = []\n",
        "    test_errors = []\n",
        "    train_sparsity = []\n",
        "    test_sparsity = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        net.train()\n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            inputs, labels = data\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            if adaptive:\n",
        "                outputs = net(inputs, epoch)\n",
        "            else:\n",
        "                outputs = net(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        # Calculate train and test error and sparsity\n",
        "        train_error = calculate_classification_error(net, trainloader, epoch, adaptive)\n",
        "        test_error = calculate_classification_error(net, testloader, epoch, adaptive)\n",
        "        train_errors.append(train_error)\n",
        "        test_errors.append(test_error)\n",
        "\n",
        "        train_sparsity.append(track_sparsity(net, trainloader, epoch, adaptive))\n",
        "        test_sparsity.append(track_sparsity(net, testloader, epoch, adaptive))\n",
        "\n",
        "        print(f'Epoch {epoch + 1}, Loss: {running_loss / len(trainloader)}, Train Error: {train_error}, Test Error: {test_error}')\n",
        "        print(f'Epoch {epoch + 1}, Train Sparsity: {train_sparsity[-1]}%, Test Sparsity: {test_sparsity[-1]}%')\n",
        "\n",
        "    return train_errors, test_errors, train_sparsity, test_sparsity\n"
      ],
      "metadata": {
        "id": "Uk07uo6fOO76"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6"
      ],
      "metadata": {
        "id": "toqE-QudOSOP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Define your data loaders (assuming they are named trainloader and testloader)\n",
        "\n",
        "# Train the NetStandard model with standard dropout\n",
        "net_standard = NetStandard(dropout_rate=0.5).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer_standard = optim.SGD(net_standard.parameters(), lr=0.001, momentum=0.9)\n",
        "train_errors_standard, test_errors_standard, train_sparsity_standard, test_sparsity_standard = train_and_record_errors(\n",
        "    net_standard, trainloader, testloader, criterion, optimizer_standard, num_epochs=10, adaptive=False)\n",
        "\n",
        "# Train the NetAdaptive model with Geometric dropout\n",
        "net_adaptive = NetAdaptive(dropout_start=0.1, dropout_end=0.5, total_epochs=10).to(device)\n",
        "optimizer_adaptive = optim.SGD(net_adaptive.parameters(), lr=0.001, momentum=0.9)\n",
        "train_errors_adaptive, test_errors_adaptive, train_sparsity_adaptive, test_sparsity_adaptive = train_and_record_errors(\n",
        "    net_adaptive, trainloader, testloader, criterion, optimizer_adaptive, num_epochs=10, adaptive=True)"
      ],
      "metadata": {
        "id": "AtZ_WoO9OS1_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}